# Optional overrides for inference endpoints
TEI_URL=http://localhost:8081
# Use Ollama for embeddings instead of TEI
# TEI_PROVIDER=ollama
# TEI_MODEL=nomic-embed-text:latest
# Optional TEI tuning
# TEI_MAX_BATCH=32
# TEI_PROMPT_NAME_QUERY=retrieval.query
# TEI_PROMPT_NAME_PASSAGE=retrieval.passage

# Skip entity extraction to speed up imports
# SKIP_ENTITY_EXTRACTION=true

# Import progress logging
# IMPORT_PROGRESS_EVERY=10
# IMPORT_PROGRESS_EVERY_SECS=5

# Entity extraction progress logging
# EXTRACT_PROGRESS_EVERY=10
# EXTRACT_PROGRESS_EVERY_SECS=5
# Log each note extraction with timing
# EXTRACT_LOG_EACH=true
# Limit the number of characters sent to extraction (0 = unlimited)
# EXTRACT_MAX_CHARS=8000

# Use Ollama for entity extraction instead of TGI
TGI_PROVIDER=ollama
TGI_URL=http://192.168.1.52:11434
TGI_MODEL=phi4-mini:latest
# Use Ollama /api/chat with JSON schema enforcement (recommended for strict JSON)
TGI_OLLAMA_USE_CHAT_SCHEMA=true
# Optional: Ask Ollama to enforce JSON-only output for extraction.
# Set to "none" to disable.
TGI_OLLAMA_FORMAT=json
# Optional: Pass Ollama generation options as JSON.
# TGI_OLLAMA_OPTIONS={"num_ctx":512,"num_predict":128,"temperature":0}
# Optional: Abort slow requests (seconds).
# TGI_OLLAMA_TIMEOUT_SECS=120
