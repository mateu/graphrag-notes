services:
  # Service 1: Semantic Embeddings (TEI)
  # Memory: ~600MB (CPU) / higher on GPU
  tei-embedding:
    build:
      context: .
      dockerfile: docker/tei-gpu/Dockerfile
    image: graphrag/tei-embedding:cuda120
    container_name: tei-embedding
    ports:
      - "8081:80"
    command: ["--model-id", "intfloat/e5-large-v2", "--port", "80", "--payload-limit", "200000000"]
    volumes:
      - ./data/tei_cache:/data
    runtime: nvidia
    environment:
      - PORT=80
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      # e5-large-v2 is 1024-dim and ships ONNX for the CPU image.

  # Service 2: Entity Extraction LLM (TGI)
  # Memory: ~3.0GB (4-bit quantized)
  tgi-extraction:
    image: ghcr.io/huggingface/text-generation-inference:3.3
    container_name: tgi-extraction
    ports:
      - "8082:80"
    command: ["--disable-custom-kernels"]
    volumes:
      - ./data/tgi_cache:/data
    environment:
      # Use an AWQ-quantized model for stability on 6GB VRAM
      - MODEL_ID=Sreenington/Phi-3-mini-4k-instruct-AWQ
      - PORT=80
      - MAX_TOTAL_TOKENS=2048
      - MAX_INPUT_TOKENS=1024
      - MAX_BATCH_PREFILL_TOKENS=512
      # AWQ quantization (no bitsandbytes runtime needed)
      - QUANTIZE=awq
      - TRUST_REMOTE_CODE=true
      # Hugging Face token (set in .env)
      - HF_TOKEN=${HF_TOKEN}
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
