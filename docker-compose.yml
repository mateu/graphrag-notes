services:
  # Service 1: Semantic Embeddings (TEI)
  # Memory: ~600MB
  tei-embedding:
    build:
      context: ./docker/tei
      args:
        CUDA_COMPUTE_CAP: "120"
    image: graphrag/tei-embedding:cc120
    container_name: tei-embedding
    ports:
      - "8081:80"
    volumes:
      - ./data/tei_cache:/data
    environment:
      - MODEL_ID=BAAI/bge-m3
      - PORT=80
      # bge-m3 is 1024-dim and TEI-compatible.
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  # Service 2: Entity Extraction LLM (TGI)
  # Memory: ~3.0GB (4-bit quantized)
  tgi-extraction:
    image: ghcr.io/huggingface/text-generation-inference:3.3
    container_name: tgi-extraction
    ports:
      - "8082:80"
    volumes:
      - ./data/tgi_cache:/data
    environment:
      # Use an AWQ-quantized model for stability on 6GB VRAM
      - MODEL_ID=pytorch/Phi-4-mini-instruct-AWQ-INT4
      - PORT=80
      - MAX_TOTAL_TOKENS=4096
      - MAX_INPUT_TOKENS=2048
      # AWQ quantization (no bitsandbytes runtime needed)
      - QUANTIZE=awq
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
